{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62047310",
   "metadata": {},
   "source": [
    "## Part A: Build a code understanding model. Upload your own custom code files to the model and ask questions based on the code file as context. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414aeb0",
   "metadata": {},
   "source": [
    "## Home work 3 :\n",
    "\n",
    "https://www.muratkoklu.com/datasets/\n",
    "\n",
    "\n",
    "Use your own data and complete the following steps. Complete the assignment within a .ipynb notebook. Submit a .zip file containing your data and the results.\n",
    "Refer to demo_03-classification for examples of setting up a custom dataset.\n",
    "Step 1. Create your own custom dataset featuring 3 custom categories of at least 100 images each\n",
    "Step 2. Split this data between 80% training and 20% test\n",
    "Step 3. Preprocess the data as you see fit\n",
    "Step 4. Create a Convolutional Neural Network model to learn about your training set \n",
    "Step 5. Make predictions on the test data and compare them to the expected categories\n",
    "Step 6: Use GoogleNet(InceptionNet) and add a LinearLayer on top of it.\n",
    "Step 7: Train the GoogleNet model and compare the accuracy with the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0542ee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of langchain.chains module, to faciliate creation of retrieval based QA system\n",
    "# RetrevialQA first retrieves teh relevant text based on query query, and then generates based on documents\n",
    "# and QA capabilities\n",
    "from langchain.chains import RetrievalQA\n",
    "# Generate and manage embeddings on HF, useful in searches, to understand\n",
    "# meanings of texts and find similar ones\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# Used for various NLP tasks, performance benefits of C++ implementation\n",
    "from langchain.llms import LlamaCpp\n",
    "# split texts into smaller chunks or segments\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# FAISS-Facebook AI Similarity Search for efficient similarity search\n",
    "# and clustering of dense vector. Usefule for quickly retrieving docs or passages\n",
    "# most relevant to query requiring fast nearest neighbor search in high dimension\n",
    "from langchain.vectorstores import FAISS\n",
    "# Load and process pdf documents\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "# Download and cache files such as model weights from HF\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# To show progress bar\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "# To profile, how long and how often each program code is executed\n",
    "import cProfile\n",
    "# if you want to run with sorting\n",
    "import re \n",
    "\n",
    "# For bleu score calculation\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8725cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load file where code exists\n",
    "loader = PyPDFDirectoryLoader(\"HW12/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228fea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='#\\xa0Mount\\xa0to\\xa0Google\\xa0Drive\\nfrom\\xa0google.colab\\xa0 import\\xa0drive\\ndrive.mount (\\'/content/drive \\')\\nMounted at /content/drive\\n#\\xa0Navigate\\xa0to\\xa0the\\xa0necessary\\xa0folder\\n%cd\\xa0\\'/content/drive/My\\xa0Drive/Data \\xa0255\\xa0Spring\\xa02024/Google\\xa0Colab/Homework3/Grapevine_ Leaves_Image_Dataset/\\'\\n/content/drive/My Drive/Data 255 Spring 2024/Google Colab/Homework3/Grapevine_Leaves_Image_Dataset\\nls\\nAk/  Ala_Idris /  Buzgulu/  Dimnit/  Grapevine_Leaves_Image_Dataset_Citation_Request.txt  Nazli/\\nImpor t TensorFlow and other libr aries \\ue313\\n#\\xa0Import\\xa0necessary\\xa0libraries\\nimport\\xa0matplotlib.pyplot\\xa0 as\\xa0plt\\nimport\\xa0numpy\\xa0as\\xa0np\\nimport\\xa0os\\xa0#\\xa0import\\xa0os\\xa0for\\xa0various\\xa0files\\xa0and\\xa0directory\\xa0relate d\\xa0operations\\nimport\\xa0PIL\\xa0#\\xa0import\\xa0python\\xa0imaging\\xa0library\\nimport\\xa0tensorflow\\xa0 as\\xa0tf\\nimport\\xa0pathlib\\xa0 #\\xa0interact\\xa0with\\xa0file\\xa0paths\\xa0and\\xa0file\\xa0systems\\n#\\xa0keras\\xa0is\\xa0a\\xa0high-level\\xa0neural\\xa0networks\\xa0API\\xa0for\\n#\\xa0training,\\xa0building\\xa0and\\xa0deploying\\xa0deep\\xa0learning\\xa0m odels\\nfrom\\xa0tensorflow\\xa0 import\\xa0keras\\nfrom\\xa0tensorflow.keras\\xa0 import\\xa0layers\\nfrom\\xa0tensorflow.keras.models\\xa0 import\\xa0Sequential\\nAbout the dataset: The gr apevine leaf dataset consists of healthy lea ves and unhealthy lea ves aff ected b y the Esca disease. Ther e are 500\\nimages within dataset. The dataset has the following folders:\\n/Ak\\n/Nazli\\n/Dimnit\\n/Buz gulu\\n/Ala_Idris\\nThe dataset is downloaded fr om the https:/ /www .mur atkoklu.com/en/publications/  website.Download and explor e the dataset \\ue313\\ndata_dir\\xa0=\\xa0pathlib.Path( \"/content/drive/My\\xa0Drive/Data\\xa0255\\xa0Spring\\xa02024/Goog le\\xa0Colab/Homework3/Grapevine_Leaves_Image_Dataset/ \")\\n#\\xa0Get\\xa0a\\xa0list\\xa0of\\xa0all\\xa0items\\xa0(files\\xa0and\\xa0directories)\\xa0 in\\xa0the\\xa0directory\\nall_items\\xa0=\\xa0os.listdir(data_dir)\\n#\\xa0Filter\\xa0out\\xa0only\\xa0the\\xa0directories\\nfolders\\xa0=\\xa0[item\\xa0 for\\xa0item\\xa0in\\xa0all_items\\xa0 if\\xa0os.path.isdir(os.path.join(data_dir,\\xa0item))]\\n#\\xa0Print\\xa0the\\xa0list\\xa0of\\xa0folders\\nprint(\"Folders\\xa0in\\xa0the\\xa0directory:\" )\\nfor\\xa0folder\\xa0 in\\xa0folders:\\n\\xa0\\xa0\\xa0\\xa0print(folder)\\nFolders in the directory:\\nAk\\nNazli\\nDimnit\\nBuzgulu\\nAla_Idris\\nAfter downloading the dataset ther e are 500 images within dataset.5/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 1/11', metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 0}), Document(page_content=\"image_count\\xa0=\\xa0 len(list(data_dir.glob( '*/*.png' )))\\nprint(image_count)\\n500\\nHere are some lea ves.\\n#\\xa0View\\xa0one\\xa0leaf\\nak_grape\\xa0=\\xa0 list(data_dir.glob( 'Ak/*'))\\nPIL.Image. open(str(ak_grape[ 0]))\\n#\\xa0View\\xa0second\\xa0leaf\\nak_grape\\xa0=\\xa0 list(data_dir.glob( 'Ak/*'))\\nPIL.Image. open(str(ak_grape[ 1]))\\nLoad using k eras.pr eprocessing \\ue3135/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 2/11\", metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 1}), Document(page_content='Load the images using off the disk using \"image_dataset_fr om_dir ectory\" utility .\\nDe\\x00ne some par ameters for the loader:Create a dataset\\ue313\\nbatch_size\\xa0=\\xa0 32\\nimg_height\\xa0=\\xa0 511\\nimg_width\\xa0=\\xa0 511\\nLet split dataset. 80% for tr aining and 20% for v alidation.\\ntrain_ds\\xa0=\\xa0tf.keras.preprocessing.image_dataset_fr om_directory(\\n\\xa0\\xa0\\xa0\\xa0data_dir,\\n\\xa0\\xa0\\xa0\\xa0validation_split= 0.2,\\n\\xa0\\xa0\\xa0\\xa0subset= \"training\" ,\\n\\xa0\\xa0\\xa0\\xa0seed= 123,\\n\\xa0\\xa0\\xa0\\xa0image_size=(img_height,\\xa0img_width),\\n\\xa0\\xa0\\xa0\\xa0batch_size=batch_size\\n)\\nFound 500 files belonging to 5 classes.\\nUsing 400 files for training.\\nval_ds\\xa0=\\xa0tf.keras.preprocessing.image_dataset_from _directory(\\n\\xa0\\xa0\\xa0\\xa0data_dir,\\n\\xa0\\xa0\\xa0\\xa0validation_split= 0.2,\\n\\xa0\\xa0\\xa0\\xa0subset= \"validation\" ,\\n\\xa0\\xa0\\xa0\\xa0seed= 123,\\n\\xa0\\xa0\\xa0\\xa0image_size=(img_height,\\xa0img_width),\\n\\xa0\\xa0\\xa0\\xa0batch_size=batch_size\\n)\\nFound 500 files belonging to 5 classes.\\nUsing 100 files for validation.\\nYou can \\x00nd the class names in the class_names attributes in dataset. These corr espond t o the dir ectory names in alphabetical or der.\\nclass_names\\xa0=\\xa0train_ds.class_names\\nprint(class_names)\\n[\\'Ak\\', \\'Ala_Idris\\', \\'Buzgulu\\', \\'Dimnit\\', \\'Nazli\\']\\nVisualiz e the data\\nHere are \\x00rst 9 images fr om the tr aining dataset.\\nplt.figure(figsize=( 10,10))\\nfor\\xa0images,\\xa0labels\\xa0 in\\xa0train_ds.take( 1):\\n\\xa0\\xa0for\\xa0i\\xa0in\\xa0range(9):\\n\\xa0\\xa0\\xa0\\xa0ax\\xa0=\\xa0plt.subplot( 3,\\xa03,\\xa0i+1)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow(images[i].numpy().astype( \"uint8\"))\\n\\xa0\\xa0\\xa0\\xa0plt.title(class_names[labels[i]])\\n\\xa0\\xa0\\xa0\\xa0plt.axis( \"off\")5/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 3/11', metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 2}), Document(page_content='Manually iter ate o ver the dataset.\\nfor\\xa0image_batch,\\xa0label_batch\\xa0 in\\xa0train_ds:\\n\\xa0\\xa0print(image_batch.shape)\\n\\xa0\\xa0print(label_batch.shape)\\n\\xa0\\xa0break\\n(32, 511, 511, 3)\\n(32,)\\nThe image_batch is a tensor of shape (32, 180, 180, 3). This is a batch of 32 images of shape (180, 180, 3) (the last dimension r efers t o the\\ncolor channels RGB). The label_batch is a tensor of shape (32,) that corr esponds t o the 32 images. Y ou call the .nump y() on image_batch and\\nlabel_batch tensors t o conv er them t o nump y.narr ay.\\nCon\\x00gur e the dataset for per formance\\nUse the buff ered pr efetching so y ou can yield data fr om disk with ha ving I/O becoming blocking/issues.T wo impor tant methods t o use:\\nDataset.cache(): method k eeps the tr aining data in memor y after loaded off disk after the \\x00rst epoch. This will ensur e dataset doesnot become\\nbottleneck while tr aining model.\\nDataset.pr efetch(): o verlaps data pr eprocessing and model ex ecution while tr aining.\\nAUTOTUNE\\xa0=\\xa0tf.data.AUTOTUNE\\ntrain_ds\\xa0=\\xa0train_ds.cache().shuffle( 1000).prefetch(buffer_size=AUTOTUNE)\\nval_ds\\xa0=\\xa0val_ds.cache().prefetch(buffer_size=AUTOT UNE)5/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 4/11', metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 3}), Document(page_content='Standar dize the data\\nThe RGB channels v alues ar e from [0, 255] r ange. This is not ideal for neur al network, should ha ve small input. Standarzie v alues t o be in [0, 1]\\nrange using Rescaling la yer.\\nnormalization_layer\\xa0=\\xa0layers.experimental.preproce ssing.Rescaling( 1./255)\\nnormalized_ds\\xa0=\\xa0train_ds. map(lambda\\xa0x,\\xa0y:\\xa0(normalization_layer(x),\\xa0y))\\nimage_batch,\\xa0labels_batch\\xa0=\\xa0 next(iter(normalized_ds))\\nfirst_image\\xa0=\\xa0image_batch[ 0]\\n#\\xa0Notice\\xa0the\\xa0pixels\\xa0values\\xa0are\\xa0now\\xa0in\\xa0`[0,1]`.\\nprint(np.min(first_image),\\xa0np. max(first_image))\\n0.078431375 1.0\\nCreate the model\\nThe model consists of thr ee conv olution la yers with a max pool la yer in each of them. Ther e is a fully connected la yer with 128 neur ons on t op\\nof it that is activ ated b y the r elu activ ation function.\\nnum_classes\\xa0=\\xa0 5\\nmodel\\xa0=\\xa0Sequential([\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Rescale\\xa0pixel\\xa0values\\xa0between\\xa00\\xa0and\\xa01\\n\\xa0\\xa0\\xa0\\xa0layers.experimental.preprocessing.Rescaling( 1./255,\\xa0input_shape=(img_height,\\xa0img_width,\\xa0 3)),\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Define\\xa0a\\xa0convolution\\xa0with\\xa016\\xa0filters\\xa0and\\xa03X3\\xa0ker nel\\n\\xa0\\xa0\\xa0\\xa0#layers.Conv2D(16,\\xa03,\\xa0padding=\\'same\\',\\xa0activation=\\' relu\\'),\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Max\\xa0pooling\\xa0layer\\xa0to\\xa0reduce\\xa0spatial\\xa0dimension\\n\\xa0\\xa0\\xa0\\xa0layers.Conv2D( 16,\\xa03,\\xa0padding= \\'same\\',\\xa0activation= \\'relu\\'),\\n\\xa0\\xa0\\xa0\\xa0layers.MaxPooling2D(),\\n\\xa0\\xa0\\xa0\\xa0layers.Conv2D( 32,\\xa03,\\xa0padding= \\'same\\',\\xa0activation= \\'relu\\'),\\n\\xa0\\xa0\\xa0\\xa0layers.MaxPooling2D(),\\n\\xa0\\xa0\\xa0\\xa0layers.Conv2D( 64,\\xa03,\\xa0padding= \\'same\\',\\xa0activation= \\'relu\\'),\\n\\xa0\\xa0\\xa0\\xa0layers.MaxPooling2D(),\\n\\xa0\\xa0\\xa0\\xa0#\\xa0Flatten\\xa0layer\\xa0to\\xa0convert\\xa0the\\xa0#D\\xa0output\\xa0to\\xa01D\\xa0out put\\xa0for\\xa0fully\\xa0connected\\xa0layer\\n\\xa0\\xa0\\xa0\\xa0layers.Flatten(),\\n\\xa0\\xa0\\xa0\\xa0layers.Dense( 128,\\xa0activation= \\'relu\\'),\\n\\xa0\\xa0\\xa0\\xa0layers.Dense(num_classes)\\n])\\nCompile model\\nmodel.compile(optimizer= \\'adam\\',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss=tf.keras.losses.SparseCategoric alCrossentropy(from_logits= True),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0metrics=[ \\'accuracy\\' ])\\nModel Summar y\\nView all the la yers of the network using model\\' s summar y\\nmodel.summary()\\nModel: \"sequential_9\"\\n_________________________________________________________________\\n Layer (type)                Output Shape              Param #   \\n=================================================================\\n rescaling_10 (Rescaling)    (None, 511, 511, 3)       0         \\n                                                                 \\n conv2d_21 (Conv2D)          (None, 511, 511, 16)      448       \\n                                                                 \\n max_pooling2d_19 (MaxPooli  (None, 255, 255, 16)      0         \\n ng2D)                                                           \\n                                                                 \\n conv2d_22 (Conv2D)          (None, 255, 255, 32)      4640      \\n                                                                 \\n max_pooling2d_20 (MaxPooli  (None, 127, 127, 32)      0         \\n ng2D)                                                           \\n                                                                 \\n conv2d_23 (Conv2D)          (None, 127, 127, 64)      18496     \\n                                                                 \\n max_pooling2d_21 (MaxPooli  (None, 63, 63, 64)        0         \\n ng2D)                                                           5/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 5/11', metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 4}), Document(page_content=\"                                                                 \\n flatten_7 (Flatten)         (None, 254016)            0         \\n                                                                 \\n dense_18 (Dense)            (None, 128)               32514176  \\n                                                                 \\n dense_19 (Dense)            (None, 5)                 645       \\n                                                                 \\n=================================================================\\nTotal params: 32538405 (124.12 MB)\\nTrainable params: 32538405 (124.12 MB)\\nNon-trainable params: 0 (0.00 Byte)\\n_________________________________________________________________\\nTrain the model\\nepochs\\xa0=\\xa0 6\\nhistory\\xa0=\\xa0model.fit(\\n\\xa0\\xa0\\xa0\\xa0train_ds,\\n\\xa0\\xa0\\xa0\\xa0validation_data\\xa0=\\xa0val_ds,\\n\\xa0\\xa0\\xa0\\xa0epochs\\xa0=\\xa0epochs\\n)\\nEpoch 1/6\\n13/13 [==============================] - 140s 11s/step - loss: 6.0466 - accuracy: 0.1725 - val_loss: 1.6031 - val_accuracy: \\nEpoch 2/6\\n13/13 [==============================] - 119s 9s/step - loss: 1.6063 - accuracy: 0.2600 - val_loss: 1.5971 - val_accuracy: 0\\nEpoch 3/6\\n13/13 [==============================] - 120s 9s/step - loss: 1.5297 - accuracy: 0.3675 - val_loss: 1.5316 - val_accuracy: 0\\nEpoch 4/6\\n13/13 [==============================] - 118s 9s/step - loss: 1.3336 - accuracy: 0.4625 - val_loss: 1.5693 - val_accuracy: 0\\nEpoch 5/6\\n13/13 [==============================] - 118s 9s/step - loss: 0.9596 - accuracy: 0.7050 - val_loss: 1.5566 - val_accuracy: 0\\nEpoch 6/6\\n13/13 [==============================] - 120s 9s/step - loss: 0.6087 - accuracy: 0.8125 - val_loss: 1.9500 - val_accuracy: 0\\nVisualiz e the r esults\\nCreate plots for accr aucy on tr aining and v alidation sets\\nacc\\xa0=\\xa0history.history[ 'accuracy' ]\\nval_acc\\xa0=\\xa0history.history[ 'val_accuracy' ]\\nloss\\xa0=\\xa0history.history[ 'loss']\\nval_loss\\xa0=\\xa0history.history[ 'val_loss' ]\\nepochs_range\\xa0=\\xa0 range(epochs)\\nplt.figure(figsize=( 8,\\xa08))\\nplt.subplot( 1,\\xa02,\\xa01)\\nplt.plot(epochs_range,\\xa0acc,\\xa0label= 'Training\\xa0Accuracy' )\\nplt.plot(epochs_range,\\xa0val_acc,\\xa0label= 'Validation\\xa0Accuracy' )\\nplt.legend(loc= 'lower\\xa0right' )\\nplt.title( 'Training\\xa0and\\xa0Validation\\xa0Accuracy' )\\nplt.subplot( 1,\\xa02,\\xa02)\\nplt.plot(epochs_range,\\xa0loss,\\xa0label= 'Training\\xa0Loss' )\\nplt.plot(epochs_range,\\xa0val_loss,\\xa0label= 'Validation\\xa0Loss' )\\nplt.legend(loc= 'upper\\xa0right' )\\nplt.title( 'Training\\xa0and\\xa0Validation\\xa0Loss' )\\nplt.show()5/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 6/11\", metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 5}), Document(page_content='predictions\\xa0=\\xa0model.predict(val_ds)\\npredicted_categories\\xa0=\\xa0[np.argmax(p)\\xa0 for\\xa0p\\xa0in\\xa0predictions]\\nprint(predicted_categories)\\ntest_loss,\\xa0test_acc\\xa0=\\xa0model.evaluate(val_ds)\\nprint(\\'Test\\xa0accuracy:\\' ,\\xa0test_acc)\\nAs y ou can see fr om the plots, the tr aining accur acy and v alidation accur acy ar e off b y a lar ge mar gin, and the model has achie ved only ar ound\\n40% accur acy on the v alidation set.\\nThis appears t o be a case of o ver\\x00tting, wher ein the data is able t o predict with good accur acy on the tr aining data, but not with good accur acy\\non unseen or v alidation data.\\nLet us tr y to incr ease the o verall per formance. Use \" data augmentation \" and \" dropout\" techniques.\\nOver\\x00tting\\ue313\\nData augmentation\\nOver\\x00tting occurs when the model is able t o gener alize on the tr aining model but not able t o gener alize on v alidation data (unseen data). Data\\naugmentation gener ates additional data fr om the tr aining sample, b y augmenting them using r andom tr anformations that yield beliv able\\nlooking images. This helps expose the model t o mor e aspects of the data and able t o gener alize better .5/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 7/11', metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 6}), Document(page_content='data_augmentation\\xa0=\\xa0keras.Sequential(\\n\\xa0\\xa0[\\n\\xa0\\xa0\\xa0\\xa0layers.experimental.preprocessing.RandomFlip( \"horizontal\" ,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0i nput_shape=(img_height,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0img_width,\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 3)),\\n\\xa0\\xa0\\xa0\\xa0layers.experimental.preprocessing.RandomRotati on(0.1),\\n\\xa0\\xa0\\xa0\\xa0layers.experimental.preprocessing.RandomZoom( 0.1),\\n\\xa0\\xa0]\\n)\\nLets visualiz e few imahes with augmented data\\nplt.figure(figsize=( 10,\\xa010))\\nfor\\xa0images,\\xa0_\\xa0 in\\xa0train_ds.take( 1):\\n\\xa0\\xa0for\\xa0i\\xa0in\\xa0range(9):\\n\\xa0\\xa0\\xa0\\xa0augmented_images\\xa0=\\xa0data_augmentation(images)\\n\\xa0\\xa0\\xa0\\xa0ax\\xa0=\\xa0plt.subplot( 3,\\xa03,\\xa0i\\xa0+\\xa01)\\n\\xa0\\xa0\\xa0\\xa0plt.imshow(augmented_images[ 0].numpy().astype( \"uint8\"))\\n\\xa0\\xa0\\xa0\\xa0plt.axis( \"off\")\\nDropout\\nTo help r educe o ver\\x00tting using dr opout, a form of r egularization.\\nWhen y ou set \\' dropout\\' t o a la yer, it randomly dr ops out (b y setting the activ ation function t o 0) a number of outputs units fr om the la yer during\\nthe tr aining pr ocess. The dr opout technique is applied only t o the tr aining set. Dr opout tak es fr actional numbers as input such as 0.1, 0.2. This\\nmeans 10%, 20% of the output units will be r andomly set t o 0.5/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 8/11', metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 7}), Document(page_content='model\\xa0=\\xa0Sequential([\\n\\xa0\\xa0data_augmentation,\\n\\xa0\\xa0layers.experimental.preprocessing.Rescaling( 1./255),\\n\\xa0\\xa0layers.Conv2D( 16,\\xa03,\\xa0padding= \\'same\\',\\xa0activation= \\'relu\\'),\\n\\xa0\\xa0layers.MaxPooling2D(),\\n\\xa0\\xa0layers.Conv2D( 32,\\xa03,\\xa0padding= \\'same\\',\\xa0activation= \\'relu\\'),\\n\\xa0\\xa0layers.MaxPooling2D(),\\n\\xa0\\xa0layers.Conv2D( 64,\\xa03,\\xa0padding= \\'same\\',\\xa0activation= \\'relu\\'),\\n\\xa0\\xa0layers.MaxPooling2D(),\\n\\xa0\\xa0layers.Dropout( 0.2),\\n\\xa0\\xa0layers.Flatten(),\\n\\xa0\\xa0layers.Dense( 128,\\xa0activation= \\'relu\\'),\\n\\xa0\\xa0layers.Dense(num_classes)\\n])\\nCompile the model\\nmodel.compile(optimizer= \\'adam\\',\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0loss=tf.keras.losses.SparseCategoric alCrossentropy(from_logits= True),\\n\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0metrics=[ \\'accuracy\\' ])\\nmodel.summary()\\nModel: \"sequential_11\"\\n_________________________________________________________________\\n Layer (type)                Output Shape              Param #   \\n=================================================================\\n sequential_10 (Sequential)  (None, 511, 511, 3)       0         \\n                                                                 \\n rescaling_11 (Rescaling)    (None, 511, 511, 3)       0         \\n                                                                 \\n conv2d_24 (Conv2D)          (None, 511, 511, 16)      448       \\n                                                                 \\n max_pooling2d_22 (MaxPooli  (None, 255, 255, 16)      0         \\n ng2D)                                                           \\n                                                                 \\n conv2d_25 (Conv2D)          (None, 255, 255, 32)      4640      \\n                                                                 \\n max_pooling2d_23 (MaxPooli  (None, 127, 127, 32)      0         \\n ng2D)                                                           \\n                                                                 \\n conv2d_26 (Conv2D)          (None, 127, 127, 64)      18496     \\n                                                                 \\n max_pooling2d_24 (MaxPooli  (None, 63, 63, 64)        0         \\n ng2D)                                                           \\n                                                                 \\n dropout_2 (Dropout)         (None, 63, 63, 64)        0         \\n                                                                 \\n flatten_8 (Flatten)         (None, 254016)            0         \\n                                                                 \\n dense_20 (Dense)            (None, 128)               32514176  \\n                                                                 \\n dense_21 (Dense)            (None, 5)                 645       \\n                                                                 \\n=================================================================\\nTotal params: 32538405 (124.12 MB)\\nTrainable params: 32538405 (124.12 MB)\\nNon-trainable params: 0 (0.00 Byte)\\n_________________________________________________________________\\nepochs\\xa0=\\xa0 15\\nhistory\\xa0=\\xa0model.fit(\\n\\xa0\\xa0train_ds,\\n\\xa0\\xa0validation_data=val_ds,\\n\\xa0\\xa0epochs=epochs\\n)\\nEpoch 1/15\\n13/13 [==============================] - 142s 11s/step - loss: 4.3783 - accuracy: 0.1925 - val_loss: 1.6082 - val_accuracy: \\nEpoch 2/15\\n13/13 [==============================] - 136s 11s/step - loss: 1.6095 - accuracy: 0.1925 - val_loss: 1.6087 - val_accuracy: \\nEpoch 3/15\\n13/13 [==============================] - 139s 11s/step - loss: 1.6026 - accuracy: 0.2800 - val_loss: 1.6063 - val_accuracy: \\nEpoch 4/15\\n13/13 [==============================] - 139s 11s/step - loss: 1.5988 - accuracy: 0.2700 - val_loss: 1.6178 - val_accuracy: \\nEpoch 5/15\\n13/13 [==============================] - 138s 11s/step - loss: 1.5739 - accuracy: 0.2750 - val_loss: 1.6103 - val_accuracy: \\nEpoch 6/155/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 9/11', metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 8}), Document(page_content=\"13/13 [==============================] - 137s 11s/step - loss: 1.5540 - accuracy: 0.3225 - val_loss: 1.5662 - val_accuracy: \\nEpoch 7/15\\n13/13 [==============================] - 139s 11s/step - loss: 1.5448 - accuracy: 0.3075 - val_loss: 1.5802 - val_accuracy: \\nEpoch 8/15\\n13/13 [==============================] - 145s 11s/step - loss: 1.5147 - accuracy: 0.3400 - val_loss: 1.5195 - val_accuracy: \\nEpoch 9/15\\n13/13 [==============================] - 139s 11s/step - loss: 1.5055 - accuracy: 0.3575 - val_loss: 1.6798 - val_accuracy: \\nEpoch 10/15\\n13/13 [==============================] - 138s 11s/step - loss: 1.4731 - accuracy: 0.3850 - val_loss: 1.4842 - val_accuracy: \\nEpoch 11/15\\n13/13 [==============================] - 138s 11s/step - loss: 1.4130 - accuracy: 0.4175 - val_loss: 1.7418 - val_accuracy: \\nEpoch 12/15\\n13/13 [==============================] - 145s 11s/step - loss: 1.3798 - accuracy: 0.4000 - val_loss: 1.3421 - val_accuracy: \\nEpoch 13/15\\n13/13 [==============================] - 139s 11s/step - loss: 1.3861 - accuracy: 0.4300 - val_loss: 1.5959 - val_accuracy: \\nEpoch 14/15\\n13/13 [==============================] - 139s 11s/step - loss: 1.2702 - accuracy: 0.5025 - val_loss: 1.4498 - val_accuracy: \\nEpoch 15/15\\n13/13 [==============================] - 138s 11s/step - loss: 1.3020 - accuracy: 0.4650 - val_loss: 1.7912 - val_accuracy: \\nVisualiz e training r esults\\nAfter applying data augmentation and Dr opout, ther e is less o ver\\x00tting than befor e, and tr aining and v alidation accur acy ar e closer aligned.\\nacc\\xa0=\\xa0history.history[ 'accuracy' ]\\nval_acc\\xa0=\\xa0history.history[ 'val_accuracy' ]\\nloss\\xa0=\\xa0history.history[ 'loss']\\nval_loss\\xa0=\\xa0history.history[ 'val_loss' ]\\nepochs_range\\xa0=\\xa0 range(epochs)\\nplt.figure(figsize=( 8,\\xa08))\\nplt.subplot( 1,\\xa02,\\xa01)\\nplt.plot(epochs_range,\\xa0acc,\\xa0label= 'Training\\xa0Accuracy' )\\nplt.plot(epochs_range,\\xa0val_acc,\\xa0label= 'Validation\\xa0Accuracy' )\\nplt.legend(loc= 'lower\\xa0right' )\\nplt.title( 'Training\\xa0and\\xa0Validation\\xa0Accuracy' )\\nplt.subplot( 1,\\xa02,\\xa02)\\nplt.plot(epochs_range,\\xa0loss,\\xa0label= 'Training\\xa0Loss' )\\nplt.plot(epochs_range,\\xa0val_loss,\\xa0label= 'Validation\\xa0Loss' )\\nplt.legend(loc= 'upper\\xa0right' )\\nplt.title( 'Training\\xa0and\\xa0Validation\\xa0Loss' )\\nplt.show()5/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 10/11\", metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 9}), Document(page_content='5/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 11/11', metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 10})]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5464a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Extracted Data into Text Chunks\n",
    "# overlap --refers to how much characters from the end of one \n",
    "# chunk of text are repeated at the beginning of the next chunk\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=15)\n",
    "\n",
    "text_chunks = text_splitter.split_documents(data)\n",
    "#text_chunks = text_splitter.split_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4745e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c97a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Ala_Idris\\nAfter downloading the dataset ther e are 500 images within dataset.5/1/24, 9:36 PM Data 255 - Homework 3.ipynb - Colab\\nhttps://colab.research.google.com/drive/1OvU2efzoIUoCPVijwbEyyg4ZGvoAcT-g#printMode=true 1/11', metadata={'source': 'HW12/Data 255 - Homework 3.pdf', 'page': 0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the third chunk\n",
    "text_chunks[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57409ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downlaod the Embeddings\n",
    "# allenai/specter: Specifically designed for scientific papers, this model might be particularly \n",
    "# effective if your documents are academic or highly technical.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "                                   #\"sentence-transformers/paraphrase-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e075b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embeddings for each of the Text Chunk\n",
    "vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3c1c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/vanikancherlapalli/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/731a9fc8f06f5f5e2db8a0cf9d256197eb6e05d1/mistral-7b-instruct-v0.1.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 8\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     4.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '8', 'llama.context_length': '32768', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '17', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'mistralai_mistral-7b-instruct-v0.1'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "# Takes about 3 minute to complete\n",
    "# Location to download the model to run locally\n",
    "# the model is downloaded to \\.cache\\huggingface\\hub folder to be accessible\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\"\n",
    "model_basename = \"mistral-7b-instruct-v0.1.Q5_K_M.gguf\"\n",
    "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)\n",
    "\n",
    "# Initliaze an instance of LlamaCpp class which is\n",
    "# a wrapper provided by LLaMA(LLM) in C++ library\n",
    "# \n",
    "llm = LlamaCpp(\n",
    "    # model can process input data in chunks, uneful when operating under memory contraints\n",
    "    streaming = True,\n",
    "    # location of the model which was just downloaded\n",
    "    model_path=model_path,\n",
    "    # Sets temperature for models output generation, it controls randomness, \n",
    "    # lower closer to 0 makes model more deterministic favoring outcome, higher temp\n",
    "    # increases diversity but can reduce coherence\n",
    "    temperature=0.25,\n",
    "    # nucleus sampling, 1=model will consider the entire set of possbile next tokens at each\n",
    "    # step generation, reducing value makes output more focussed and less random\n",
    "    top_p=1,\n",
    "    # print addtional message such as debugging, messaging, logs\n",
    "    verbose=True,\n",
    "    # context size the model can handle in terms of tokens, large size\n",
    "    # allows for lenghty inputs\n",
    "    n_ctx=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "116479c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize langchain\n",
    "# RetrievalQA - for question answering against an index; llm is the based model\n",
    "# chain_type - type of chain to use --The stuff documents chain (â€œstuff\" as in \"to stuff\" or \"to fill\") \n",
    "# It takes a list of documents, inserts them all into a prompt, and passes that prompt to an LLM.\n",
    "# k=2 means the top two closest vectors are returned, which could be used to fetch the most \n",
    "# relevant documents or pieces of information in response to a query.\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(search_kwargs={\"k\": 4}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aed6dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, chat_history):\n",
    "    \"\"\"\n",
    "    Processes a single query using a QA system and updates the chat history.\n",
    "    \n",
    "    Parameters:\n",
    "    - query (str): The query string to be processed by the QA system.\n",
    "    - chat_history (list of dict): The chat history that maintains context across queries.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The answer corresponding to the query.\n",
    "    \"\"\"\n",
    "    # Prepare the input as a single dictionary if the QA system expects it\n",
    "    input_data = {\n",
    "        'query': query,\n",
    "        'chat_history': chat_history\n",
    "    }\n",
    "\n",
    "    # Update chat history with the current query\n",
    "    chat_history.append({'role': 'user', 'content': query})\n",
    "    \n",
    "    # 'invoke' method that can accept structured input\n",
    "    answer = qa.invoke(input_data)  # Pass a single dictionary as an argument\n",
    "    \n",
    "    # Update chat history with the provided answer\n",
    "    chat_history.append({'role': 'assistant', 'content': answer['result']})\n",
    "    \n",
    "    # Return only the result part of the answer\n",
    "    return answer['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f4d76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize chat history\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fb5c8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answers to the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b50138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    7619.93 ms\n",
      "llama_print_timings:      sample time =      12.85 ms /   106 runs   (    0.12 ms per token,  8249.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =  143342.05 ms /  1643 tokens (   87.24 ms per token,    11.46 tokens per second)\n",
      "llama_print_timings:        eval time =    9792.93 ms /   105 runs   (   93.27 ms per token,    10.72 tokens per second)\n",
      "llama_print_timings:       total time =  153626.02 ms /  1748 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Summarize what the program is doing\n",
      "Answer:  The program is loading and preparing a dataset for training a machine learning model using TensorFlow. It first loads the data from disk and counts the number of images in the dataset. It then manually iterates over the dataset to view the shape of the image and label tensors. Next, it configures the dataset for performance by enabling buffered pr efetching and caching the training data in memory. Finally, it trains the model using the configured dataset and visualizes the accuracy and loss on the training and validation sets.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the query (directly as a string)\n",
    "query12 = \"Summarize what the program is doing?\"\n",
    "\n",
    "# Process the query and print the results\n",
    "answer = process_query(query12, chat_history)\n",
    "print(f\"Query: {query12}\\nAnswer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8b5d113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7619.93 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /    12 runs   (    0.11 ms per token,  9070.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =  127292.27 ms /  1511 tokens (   84.24 ms per token,    11.87 tokens per second)\n",
      "llama_print_timings:        eval time =    1010.09 ms /    11 runs   (   91.83 ms per token,    10.89 tokens per second)\n",
      "llama_print_timings:       total time =  128592.49 ms /  1522 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How many images are in the program?\n",
      "Answer:  There are 500 images in the dataset.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the query (directly as a string)\n",
    "query2 = \"How many images are in the program?\"\n",
    "\n",
    "# Process the query and print the results\n",
    "answer = process_query(query2, chat_history)\n",
    "print(f\"Query: {query2}\\nAnswer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0dbe2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7619.93 ms\n",
      "llama_print_timings:      sample time =       4.44 ms /    37 runs   (    0.12 ms per token,  8342.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   53117.18 ms /   648 tokens (   81.97 ms per token,    12.20 tokens per second)\n",
      "llama_print_timings:        eval time =    3290.55 ms /    37 runs   (   88.93 ms per token,    11.24 tokens per second)\n",
      "llama_print_timings:       total time =   56605.93 ms /   685 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Are there any visualizations in the program?\n",
      "Answer:  Yes, there are two visualizations in the program. The first one shows the training and validation loss over time, while the second one shows the training and validation accuracy over time.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the query (directly as a string)\n",
    "query3 = \"Are there any visualizations in the program?\"\n",
    "\n",
    "# Process the query and print the results\n",
    "answer = process_query(query3, chat_history)\n",
    "print(f\"Query: {query3}\\nAnswer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8d21b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7619.93 ms\n",
      "llama_print_timings:      sample time =       1.99 ms /    17 runs   (    0.12 ms per token,  8542.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   98919.77 ms /  1170 tokens (   84.55 ms per token,    11.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1494.51 ms /    16 runs   (   93.41 ms per token,    10.71 tokens per second)\n",
      "llama_print_timings:       total time =  100663.26 ms /  1186 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How many lines of code in the program?\n",
      "Answer:  The number of lines of code in a program is not relevant to the question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the query (directly as a string)\n",
    "query4 = \"How many lines of code in the program?\"\n",
    "\n",
    "# Process the query and print the results\n",
    "answer = process_query(query4, chat_history)\n",
    "print(f\"Query: {query4}\\nAnswer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "755c8f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    7619.93 ms\n",
      "llama_print_timings:      sample time =      13.50 ms /   119 runs   (    0.11 ms per token,  8815.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =  203872.39 ms /  2390 tokens (   85.30 ms per token,    11.72 tokens per second)\n",
      "llama_print_timings:        eval time =   11819.96 ms /   118 runs   (  100.17 ms per token,     9.98 tokens per second)\n",
      "llama_print_timings:       total time =  216370.20 ms /  2508 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: any suggestions to make the program better?\n",
      "Answer: \n",
      "\n",
      "To improve the performance of the model, you can try using data augmentation and dropout techniques. Data augmentation can generate additional data from the training samples by applying random transformations such as rotation, flipping, and zooming. Dropout can randomly drop out some neurons during training to prevent overfitting. You can also try increasing the number of epochs or changing the learning rate to see if that improves the performance. Additionally, you may want to consider using a different model architecture or hyperparameters such as batch size, optimizer, and regularization techniques.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the query (directly as a string)\n",
    "query5 = \"any suggestions to make the program better?\"\n",
    "\n",
    "# Process the query and print the results\n",
    "answer = process_query(query5, chat_history)\n",
    "print(f\"Query: {query5}\\nAnswer: {answer}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral0404",
   "language": "python",
   "name": "mistral0404"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
